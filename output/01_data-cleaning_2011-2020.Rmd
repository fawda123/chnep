---
title: "CHNEP Water Quality Data Cleaning"
author: "Miles Medina PhD and Christine Angelini PhD, University of Florida Center for Coastal Solutions"
date: "April 1, 2022"
output:
  html_document:
    theme: sandstone
    highlight: haddock
---

# Contents {#top}

  * [Overview](#Overview)
  * [Load R libraries and data](#Load)
  * [Specify data classes](#Classes)
  * [Subset data by date](#Dates)
  * [Clean up factor levels and redundancies](#Factors)
  * [Abbreviate analyte names](#Analytes)
  * [Standardize measurement units](#Units)
  * [Remove field blanks and equipment blanks](#Blanks)
  * [Standardize sample depth units](#DepthUnits)
  * [Subset surface sample data](#Depths)
  * [Remove duplicate records](#Dupes)
  * [Associate stations with strata](#Strata)
  * [Remove records flagged with fatal qualifier codes](#Qualifiers)
    + [Qualifier codes and definitions](#Qualifiers1)
    + [Fatal qualifier codes](#Qualifiers2)
  * [Detection limits and non-detects](#MDL)
    + [Rule 1](#MDL1)
    + [Rule 2](#MDL2)
    + [Rule 3](#MDL3)
    + [Rule 4](#MDL4)
  * [Apply thresholds to result values](#Thresholds)
  * [Export clean data](#Export)
  * [References](#Refs)
  

# Overview {#Overview}
  
CHNEP contracted the UF Center for Coastal Solutions (CCS) to perform a water quality trend analysis and visualization for the CHNEP Water Atlas, which is maintained by USF. The first task of this project is to clean the water quality data. The data were originally collected by various agencies, compiled and processed by the USF Water Atlas team, and provided to CCS by USF for further cleaning. The period for trend analysis is 10 years, and as such the data cleaning procedure cleans the data from 2011 through 2020. This R Markdown report walks through the accompanying data cleaning script (R file) and its outputs. The script was implemented in R Version 4.1.0 (R Core Team, 2021).

The dataset provided to CCS reflects several data cleaning procedures performed by the USF Water Atlas team. These procedures are summarized below, and further detail can be found in the USF documentation.

  * Standardizing measurement units.
  * Discarding data flagged with fatal qualifier codes.
  * Discarding duplicate records.
  * Discarding records at sampling depths of more than 1 m.
  * Discarding records associated with field blanks and equipment blanks.
  * Assigning the method detection limit (MDL) as the reported value for non-detect samples.
  * Discarding records with reported values outside of specified minimum and maximum values (thresholds).

The data cleaning script implemented by CCS verifies the cleaning procedures listed above and applies additional procedures to enhance data quality. The major innovations/novelties associated with these procedures include

  * Standardizing and abbreviating analyte names.
  * Standardizing measurement units and sample depth units, and assigning units where they are missing.
  * Investigating qualifier codes used by each data source to ensure consistency.
  * Implementation of functions for automated recognition of fatal qualifier codes and non-detect samples, even when the qualifier code string contains multiple characters.
  * Standardizing MDLs and applying thoughtful rules for treating non-detect samples that are missing MDL information.
  * Specifying an expanded set of value thresholds, to screen unreliable data.
  
The data cleaning procedures implemented by CCS embody a conservative approach that errs on the side of retaining some unreliable data points for the sake of retaining as much high-quality data as possible, as opposed to a strict approach that would sacrifice reliable data points to more aggressively remove unreliable data. The conservative approach is appropriate in the context of non-parametric trend analysis, since non-parametric procedures are robust to outliers and can tolerate some low-quality data. Some of the procedures, particularly those involving assignment of missing measurement units or sample depth units, effectively retain records that might otherwise be unnecessarily discarded. These procedures make certain assumptions that are informed by the distributions of observed data and by domain knowledge, and these assumptions are described in the appropriate sections below.

[top](#top)


# Load R libraries and data {#Load}

First, we clear the global R environment and load libraries: *plyr* (Wickham, 2011), *dplyr* (Wickham et al., 2021), *tidyr* (Wickham, 2021), and *data.table* (Dowle & Srinivasan, 2021). The code automatically installs the packages if they are not already stored locally.
```{r message=FALSE, warning=FALSE}
rm(list=ls(all=TRUE)) 
if(!require(plyr)) { install.packages('plyr') } ;  library(plyr)
if(!require(dplyr)) { install.packages('dplyr') } ;  library(dplyr)  
if(!require(tidyr)) { install.packages('tidyr') } ;  library(tidyr) 
if(!require(data.table)) { install.packages('data.table') } ;  library(data.table)
```
Next, we load sample data, strata information, and qualifier code definitions from files provided by USF Water Atlas, as well as a file containing result value thresholds specified by CCS.
```{r}
setwd('C:\\Users\\miles.medina\\Documents\\UF Postdoc\\CHNEP\\USF docs')
dat <- read.table('Trend Analysis - CHNEP - 2020 - Export.txt',sep="|",header=TRUE,quote="",fill=TRUE)
strata <- read.csv('CHNEPstationstoGrids.csv')
code.defs <- read.csv('WA-Quality-Assurance-Code-Definitions_3_csv.csv')
thresholds <- read.csv("thresholds.csv")
```
The **dat** dataframe contains `r nrow(dat)` rows and `r ncol(dat)` columns, whose names are printed below.
```{r}
colnames(dat)
```
The **strata** dataframe associates individual sample locations (**STATIONID**) with a larger sampling area (**Stratum**). Below, the first six **strata** records are displayed.
```{r}
head(strata)
```
The **code.defs** and **thresholds** files are described later in the script.

[top](#top)


# Specify data classes {#Classes}

Next, we store the **dat** dataframe as a new dataframe object **df** and specify the type (class) of data in some of the columns. 
```{r}
df <- dat
df$DataSource <- df$DataSource %>% as.factor()
df$StationID  <- df$StationID %>% as.factor()
df$Activity_Type <- df$Activity_Type %>% as.factor()
df$Activity_Start_Date <- df$Activity_Start_Date %>% as.Date(format="%Y-%m-%d")
df$RelativeDepth       <- df$RelativeDepth %>% as.factor()
df$Characteristic <- df$Characteristic %>% as.factor()
df$Result_Unit <- df$Result_Unit %>% as.factor()
df$Activity_Depth_Unit <- df$Activity_Depth_Unit %>% as.factor()
```

[top](#top)


# Subset data by date {#Dates}

The trend analysis (Task 2 of this project) will use 10 years of sample data from 2011 through 2020. Below, we subset the data in **df** by date, store this subset as a new dataframe **df1**, and rename the **Activity_Start_Date** column to **Date**.
```{r}
min.date <- as.Date('2011-01-01')
max.date <- as.Date('2020-12-31')
df1 <- df[ which( df$Activity_Start_Date >= min.date & df$Activity_Start_Date <= max.date) ,]
colnames(df1)[which(colnames(df1)=='Activity_Start_Date')] <- 'Date'
```

[top](#top)


# Clean up factor levels and redundancies {#Factors}

Subsetting the data by date potentially removed instances of some factor levels from *factor* columns of **df1**. In this section, we check for redundancies in each *factor* column and remove any unused factor levels.
  
First, we explore the **DataSource** column, which contains `r length(unique(df1$DataSource))` unique values. The logical test below indicates that the number of factor levels is not equal to the actual number of unique values that appear in the column, so we remove unused factor levels by calling *droplevels()*.
```{r}
length(levels(df1$DataSource)) == length(unique(df1$DataSource))
df1$DataSource <- droplevels(df1$DataSource)
```
Likewise, we check the **StationID**, **Activity_Type**, **RelativeDepth**, and **Result_Unit** columns and remove unused factor levels as needed.
```{r}
length(levels(df1$StationID)) == length(unique(df1$StationID))
```
```{r}
df1$StationID <- droplevels(df1$StationID)
```
```{r}
length(levels(df1$Activity_Type)) == length(unique(df1$Activity_Type))
```
```{r}
df1$Activity_Type <- droplevels(df1$Activity_Type)
```
```{r}
length(levels(df1$RelativeDepth)) == length(unique(df1$RelativeDepth))
```
```{r}
    length(levels(df1$Result_Unit)) == length(unique(df1$Result_Unit))
```
Next, we explore the **Activity_Depth_Unit** column, which indicates the sampling depth associated with each record. The sampling depths are variously reported in meters ("m"), feet ("ft"), or with unlabeled units (""). We want depths to be expressed consistently in meters, but we will come back to this issue later. For now, we remove the unused factor levels.
```{r}
unique(df1$Activity_Depth_Unit) %>% as.character()
length(levels(df1$Activity_Depth_Unit)) == length(unique(df1$Activity_Depth_Unit))
df1$Activity_Depth_Unit <- droplevels(df1$Activity_Depth_Unit)
```
The **Characteristic** column indicates the analyte associated with each record, and there are several redundancies in these labels (due to capitalization).
```{r}
unique(df1$Characteristic) %>% sort()
```
We standardize **Characteristic** factor levels and remove unused levels.
```{r}
df1$Characteristic[ which(df1$Characteristic=='Temperature, Water') ] <- 'Temperature, water'
df1$Characteristic[ which(df1$Characteristic=='Dissolved Oxygen Saturation') ] <- 'Dissolved oxygen saturation'
df1$Characteristic[ which(df1$Characteristic=='Fecal Coliform') ] <- 'Fecal coliform'
df1$Characteristic[ which(df1$Characteristic=='Specific Conductance') ] <- 'Specific conductance'
df1$Characteristic <- droplevels(df1$Characteristic)
length(levels(df1$Characteristic)) == length(unique(df1$Characteristic))
```
After cleaning up the **Characteristic** factor levels, we find that **df1** includes data on `r length(unique(df1$Characteristic))` unique analytes. 
```{r}
unique(df1$Characteristic) %>% sort()
```

[top](#top)


# Abbreviate analyte names {#Analytes}

In a new **Analyte** column, we specify abbreviated analyte names (e.g. TN) corresponding to labels in the **Characteristic** column. We call the *plyr::mapvalues()* function to handle the assignment of the abbreviated **Analyte** names. The abbreviated names will make it easier to refer to analytes later in this script, and they can be used to label plot axes. 
```{r}
analytes.old <- levels(df1$Characteristic)
analytes.new <- c( 'Color', 'BOD', 'Chl-a', 'DO Conc', 'DO Sat',  'Fecal', 'TN',  'NH4',  'TKN',
                   'NOx',   'pH',   'TP',   'PO4',     'Sp Cond', 'Temp',  'TSS', 'Turbidity'     )
df1$Analyte <- mapvalues( df1$Characteristic, analytes.old, analytes.new ) 
analytes <- unique( df1$Analyte ) %>% sort() %>% as.character()
analytes
```
Below, we display selected columns from a random set of five records in **df1**. Note that the **Characteristic** labels correspond to the shorter **Analyte** labels.
```{r}
set.seed(32813)
df1[sample(1:nrow(df1),5),c('Date','Characteristic','Analyte','Result_Value')]
```

[top](#top)


# Standardize measurement units {#Units}

We explore the measurement units to ensure consistency. We start by printing the unique values in the **Result_Unit** column.
```{r}
unique(df$Result_Unit)
```
We print the units associated with each analyte. Each line of the output lists an analyte and an associated unit in the **Result_Unit** column; analytes associated with multiple **Result_Unit** values appear on multiple lines. Below, each analyte is associated with a single measurement unit, but the output shows several analytes associated with a blank space, indicating records with missing units (e.g. *Chl-a*).
```{r}
for( i in 1:length(levels(df1$Analyte)) ){
  this.analyte <- levels(df1$Analyte)[i]
  this.units   <- unique( df1$Result_Unit[which(df1$Analyte==this.analyte)] )
  cat( paste0( this.analyte, ' -- ', this.units, '\n' ) )
} 
```
Next, we determine whether it is reasonable to assign **Result_Unit** labels where they are missing. USF Water Atlas indicated that the dataset uses consistent measurement units for each analyte. Using histograms, we can test this assertion by comparing the distributions and ranges of the each analyte's data with and without unit labels.

We generate histograms for the data that are missing unit labels. The minimum and maximum concentrations and the number of observations are printed in the top right corner of each histogram.
```{r fig.width=10, fig.height=8}
analytes.unlabeled <- unique( df1$Analyte[which(df1$Result_Unit=='')] )
analytes.unlabeled.idx <- which( analytes %in% analytes.unlabeled )

par(mfrow=c(4,2))
  for( i in analytes.unlabeled.idx ){
    this.analyte <- analytes[i]
    this.unit    <- '?????'
    # cat(paste0(i,'. ',this.analyte,": ",this.unit,'\n'))
    if(this.unit[1]=="None"){ this.unit<-'' }
    hist.data <- df1$Result_Value[ which(df1$Analyte==this.analyte & df1$Result_Unit=='') ]
    if(length(hist.data)==0){ next }
    hist( hist.data, breaks=100,
          border=rgb(1,1,1,1), col=rgb(0,0.2,0.9,1),
          main=this.analyte, xlab=this.unit
    )
    legend('topright',bty='n',legend=( c(paste('min:',min(hist.data)),
                                         paste('max:',max(hist.data)),
                                         paste('n:',length(hist.data))
    ))
    )
  }
```
We also generate histograms for the data that include unit labels. Comparing the histograms and ranges below with the ones above, we see that they are consistent with the assertion by USF Water Atlas: The distribution of each analyte's unlabeled data is well within the range of the corresponding labeled data.
```{r fig.width=10, fig.height=8}
par(mfrow=c(4,2))
  for( i in analytes.unlabeled.idx ){
    this.analyte <- analytes[i]
    this.unit    <- unique( df1$Result_Unit[ which(df1$Analyte==this.analyte) ] )
    # cat(paste0(i,'. ',this.analyte,": ",this.unit,'\n'))
    if(this.unit[1]=="None"){ this.unit<-'' }
    this.unit <- this.unit[ which( !(this.unit %in% '' ) ) ] %>% as.character()
    hist.data <- df1$Result_Value[ which(df1$Analyte==this.analyte & df1$Result_Unit==this.unit) ]
    hist( hist.data, breaks=100,
          border=rgb(1,1,1,1), col=rgb(0.9,0.2,0,1),
          main=this.analyte, xlab=this.unit
    )
    legend('topright',bty='n',legend=( c(paste('min:',min(hist.data)),
                                         paste('max:',max(hist.data)),
                                         paste('n:',length(hist.data))
                                      ))
           )
  }
```
By checking which data sources are associated with unlabeled units, we find that the records that are missing unit labels are exclusively from the data source *LEE_PONDWATCH_WQ*.
```{r}
for( i in analytes.unlabeled.idx){
  this.analyte <- analytes[i]
  this.sources <- unique( df1$DataSource[ which(df1$Analyte==this.analyte &
                                                df1$Result_Unit=='')] )
  cat(paste0(i,'. ',this.analyte,": ",this.sources,'\n'))
}
```
We proceed under the assumption that each analyte's values are expressed in a consistent measurement unit throughout the dataset, whether or not the units are labeled. The code block below assigns unit labels where they are missing (**Result_Unit** column), allowing us to retain `r length(which(df1$Result_Unit==''))` records with unlabeled units. Inside the for-loop, we identify the measurement unit associated with each analyte and assign that unit to records that are missing a unit label.  
```{r}
for( i in analytes.unlabeled.idx ){
  this.analyte <- analytes[i]
  this.unlabeled.idx <- which( df1$Analyte==this.analyte & df1$Result_Unit=='' ) # unlabeled unit indices
  this.units <- unique(df1$Result_Unit[which(df1$Analyte==analytes[i])])  # unit labels
  replacement.unit <- this.units[ which( !(this.units %in% '' ) ) ] %>% as.character() # extract unit string
  if( length(replacement.unit)>1 ){ stop('More than one replacement unit found.') }  # error if >1 string
  df1$Result_Unit[this.unlabeled.idx] <- replacement.unit  # replace blank unit with string
}
```
Nitrogen and chlorophyll concentrations are expressed in ug/l. For consistency, we convert the units for phosphate concentrations from mg/l to ug/l.
```{r}
df1$Result_Value[ which(df1$Analyte=='PO4') ] <- df1$Result_Value[ which(df1$Analyte=='PO4') ]*1000
df1$Result_Unit[ which(df1$Analyte=='PO4') ]  <- 'ug/l'
```
Finally, we confirm that all units are labeled, and we drop unused factor levels in the **Result_Unit** column.
```{r}
which(df1$Result_Unit=='')
df1$Result_Unit <- droplevels( df1$Result_Unit )
```

[top](#top)


# Remove field blanks and equipment blanks {#Blanks}

The dataset includes several sample types (**Activity_Type** column). Field blanks and equipment blanks are included in the dataset for quality control purposes, but these blanks are not true samples.
```{r}
unique(df1$Activity_Type) %>% sort()
```
We remove the blanks and drop the associated factor levels.
```{r}
df1 <- df1[ -which( df1$Activity_Type %in% c('Equipment Blank','Field Blank')), ] 
df1$Activity_Type <- df1$Activity_Type %>% droplevels() 
```

[top](#top)


# Standardize sample depth units {#DepthUnits}

The **Activity_Depth** and **Activity_Depth_Unit** columns indicate the depths at which samples were collected. Here, we check the depth units for consistency.

First, we create new columns for depth and depth units.
```{r}
df1$Depth <- df1$Activity_Depth
df1$Depth_Unit <- df1$Activity_Depth_Unit
```
Depths are expressed in both feet and meters and some reported depths are missing units:
```{r}
unique(df1$Depth_Unit) %>% as.character()
```
We explore the depth units reported by those data sources that reported at least some depths with no units. We find that *SARASOTA_COASTALCREEK_WQ* and *STORET_21FLCHAR* reported depths in both feet and meters and with missing units. Also, none of the records from *SWFWMD_HYDRO* and *USGS_NWIS* indicate depth units. Other data sources with some missing depth units elsewhere reported depths in meters ("m").
```{r}
    no.depth.stations <- unique( df1$DataSource[ which(df1$Depth_Unit=='') ] )  # Stations with missing depth units
    for( i in 1:length(no.depth.stations) ){  
      this.units <- unique( df1$Depth_Unit[ which(df1$DataSource==no.depth.stations[i]) ] )
      cat( paste0( no.depth.stations[i], ': ', this.units, '\n' ) )
    }  # // end i loop
```
First, we standardize the reported depth units by converting feet to meters.
```{r}
df1$Depth[ which(df1$Depth_Unit=='ft') ] <- df1$Depth[ which(df1$Depth_Unit=='ft') ] * 0.3048  # convert feet to meters
df1$Depth_Unit[ which(df1$Depth_Unit=='ft') ] <- 'm'  # correct units to meters
unique(df1$Depth_Unit) %>% as.character()  # depths are expressed in meters or missing units
```
Next, we further explore depths that are missing unit labels. We find that `r length(which(df1$Depth_Unit==''))` **Depth** values are missing units, representing `r length(unique(df1$DataSource[which(df1$Depth_Unit=='')]))` data sources and `r length(unique(df1$StationID[which(df1$Depth_Unit=='')]))` stations.
```{r}
df1$DataSource[which(df1$Depth_Unit=='')] %>% unique()  # data sources with unitless depths
df1$StationID[which(df1$Depth_Unit=='')] %>% unique()  # stations with unitless depths
```
We plot the depths with and without unit labels to compare their distributions.
```{r fig.width=10, fig.height=8}
par(mfrow=c(3,2))
  # Without units
  df1$Depth[which(df1$Depth_Unit=='')] %>%
    hist(breaks=12,freq=F,col=rgb(0,0.4,0.9,0.4),border=rgb(1,1,1,1),
         main='Depths reported without units',xlab='depth (???)')
  df1$Depth[which(df1$Depth_Unit=='')] %>% sort() %>%
    plot(pch=16,col=rgb(0,0.4,0.9,0.4),cex=0.5,
         main='Depths reported without units',xlab='No. of samples',ylab='depth (???)')
    abline( h=seq(0,6,0.5), col=rgb(0,0,0,0.3) )
  # With units (0-6 m)
  df1$Depth[which(df1$Depth_Unit!='' & df1$Depth<=6)] %>%
    hist(breaks=12,freq=F,col=rgb(0.9,0.2,0.1,0.4),border=rgb(1,1,1,1),
         main='Depths reported with units (0-6 m)',xlab='depth (m)')
  df1$Depth[which(df1$Depth_Unit!='' & df1$Depth<=6)] %>% sort() %>%
    plot(pch=16,col=rgb(0.9,0.2,0.1,0.4),cex=0.5,
         main='Depths reported with units (0-6 m)',xlab='No. of samples',ylab='depth (m)')
    abline( h=seq(0,6,0.5), col=rgb(0,0,0,0.3) )
  # With units (all)
  df1$Depth[which(df1$Depth_Unit!='')] %>%
      hist(breaks=25,freq=F,col=rgb(0.9,0.2,0.1,0.4),border=rgb(1,1,1,1),
           main='Depths reported with units (all)',xlab='depth (m)')
  df1$Depth[which(df1$Depth_Unit!='')] %>% sort() %>%
      plot(pch=16,col=rgb(0.9,0.2,0.1,0.4),cex=0.5,
           main='Depths reported with units (all)',xlab='No. of samples',ylab='depth (m)')
  abline( h=seq(0,30,1), col=rgb(0,0,0,0.3) )
```
In the figure above, histograms on the left-hand side show sampling depths *without* depth units specified (blue, top row) and *with* depth units specified (red, middle and bottom rows). On the right-hand side, the corresponding sample depth values are sorted (in increasing order) to convey the number of samples at each depth. Plots in the middle row show labeled depths between 0 and 6 meters, and plots in the bottom row show all labeled depths. Among labeled depths, the values are predominantly equal to or less than 1 meter, indicating surface samples. The unlabeled depth values are also predominantly equal to or less than 1 unit, but a relatively large proportion of the samples nonetheless show depth values between 1 and 3 units.

The distributions of labeled and unlabeled depths thus appear to be quite different. Because the trend analysis and visualization (Tasks 2 and 3 of this project) will include data from surface samples only (up to 1 meter of depth), we must decide whether to assign a label of "ft" or "m" to unlabeled **Depth** values. The decision will determine which of these samples are ultimately included in the trend analysis.

Below, we compare the distribution of labeled **Depth** values to the distributions of unlabeled **Depth** values in their raw form (representing the assumption that they are expressed in *meters*) and multiplied by the conversion factor 0.3048 (representing the assumption that they are expressed in *feet*). Assuming feet for unlabeled depths generates a distribution (right panel) that more closely resembles the distribution of labeled depths (left panel), whereas assuming meters generates a distribution (middle panel) that is quite different. We therefore proceed under the assumption that unlabeled **Depth** values are expressed in feet. Assuming feet means that we retain records associated with raw unlabeled **Depth** values of about 3.3 or less. Assuming meters would cause us to discard a greater number of records (records with raw unlabeled **Depth** values >1).
```{r fig.width=10, fig.height=4}
par(mfrow=c(1,3))
# With units (0-6 m)
df1$Depth[which(df1$Depth_Unit!='' & df1$Depth<=6)] %>%
  hist(breaks=12,freq=F,col=rgb(0.9,0.2,0.1,0.4),border=rgb(1,1,1,1),
       main='Depths reported with units (0-6 m)',xlab='depth (m)')
# Without units (assuming meters)
df1$Depth[which(df1$Depth_Unit=='')] %>%
    hist(breaks=12,freq=F,col=rgb(0,0.4,0.9,0.4),border=rgb(1,1,1,1),
         main='Depths reported without units (assume meters)',xlab='depth (as m)')
# Without units (assuming feet)
(df1$Depth[which(df1$Depth_Unit=='')]*0.3048) %>%
    hist(breaks=6,xlim=c(0,6),freq=F,col=rgb(0,0.4,0.9,0.4),border=rgb(1,1,1,1),
         main='Depths reported without units (assume feet)',xlab='depth (as m, converted from ft)')
```
Below, we convert **Depth** values with missing units (**Depth_Unit**) from feet to meters, assign "m" over the full **Depth_Unit** column and remove unused factor levels.    
```{r}
missing.depth.idx <- which( !is.na(df1$Activity_Depth) & df1$Depth_Unit=='' )
df1$Depth[ missing.depth.idx ] <- df1$Depth[ missing.depth.idx ] * 0.3048  # convert (assumed) feet to meters
df1$Depth_Unit[ missing.depth.idx ] <- as.factor('m')  # Assign 'm' to entire Depth_Unit column
df1$Depth_Unit <- droplevels( df1$Depth_Unit )  # drop unused levels
```
We can verify the above operation by printing a random subset of the affected rows.
```{r}
set.seed(3994)
df1[ sample(missing.depth.idx,10), c('Date','Activity_Depth','Activity_Depth_Unit','Depth','Depth_Unit') ]
```

[top](#top)


# Subset surface sample data {#Depths}

The trend analysis will consider only surface sample data. Samples are considered to be from the surface if:

  * The **Depth** value is 1 meter or less; or
  * The **Depth** value is not reported and the **RelativeDepth** label is "Surface"; or
  * Neither the **Depth** value nor the **RelativeDepth** label are reported.
  
We subset the data by sample depth (**Depth** and **RelativeDepth** columns):
```{r}
surface.idx <- which( df1$Depth<=1 |   # reported depth no more than one meter, OR 
                        ( df1$RelativeDepth=='Surface' & is.na(df1$Depth)) | # labeled 'Surface' with no depth reported, OR
                        ( df1$RelativeDepth=='' & is.na(df1$Depth) )  )  # no depth info reported
df1.nonsurface <- df1[ -surface.idx, ]
df1 <- df1[ surface.idx, ]
```
A total of `r nrow(df1.nonsurface)` non-surface records are discarded. Below, we explore the **df1.nonsurface** dataframe, which contains all records discarded as non-surface samples, to ensure that no records are unnecessarily discarded. Although some "Surface" records were discarded, as indicated below, the associated **Depth** values exceed 1 m.
```{r}
df1.nonsurface %>% nrow()  # 81k total records
df1.nonsurface[ which(df1.nonsurface$Depth<=1), ] %>% nrow()  # zero records with Depth<=1
df1.nonsurface$RelativeDepth %>% unique() %>% as.character()  # includes some "Surface" records
df1.nonsurface[ which( df1.nonsurface$RelativeDepth=="Surface"), ] %>% nrow()  # 14.5k "Surface" records
df1.nonsurface$Depth[ which( df1.nonsurface$RelativeDepth=="Surface") ] %>% range()  # min value is >1 m
df1.nonsurface$Depth[ which( df1.nonsurface$RelativeDepth=="") ] %>% range()  # min value is >1 m
```

[top](#top)


# Remove duplicate records {#Dupes}

We call the function *duplicated()* to search for duplicate records in **df1**. The resulting **dupes** matrix lists the duplicates' **df1** row indices.
```{r}
dupes.1 <- which(duplicated(df1))  # find duplicate entries 
dupes.2 <- which(duplicated(df1,fromLast=TRUE))  # find the corresponding first entries
dupes <- cbind(dupes.1,dupes.2)  # table of duplicate rows
dupes
```
We found `r nrow(dupes)` duplicate records. Printing the duplicate pairs confirms they are duplicates:
```{r}
apply( dupes, 1, function(x) df1[ c(x[1],x[2]), ] )
```
We remove the duplicate records and store the result in a new dataframe **df2**. Calling *duplicated()* confirms that **df2** contains no duplicates. 
```{r}
df2 <- df1[-dupes.1,]  
which(duplicated(df2))  # confirm no duplicates 
```  

[top](#top)


# Associate stations with strata {#Strata}

At the top of this script, we loaded strata information from a csv file. We call the *plyr::mapvalues()* function to associate **StationID** values (in **df2**) with **Stratum** values (in **strata**), and we store the results in a new **Stratum** column in **df2**. Some monitoring stations are not part of strata, and when a **StationID** value in **df2** does not appear in the **strata** dataframe, the **StationID** value is copied to the **Stratum** column in **df2**. Subsequent checks confirm that each station is associated with only one strata (each strata can be associated with one or more stations).
```{r}
# Create new column grouping stations by stratum (not all 'strata' stations appear in 'df2')
df2$Stratum <- mapvalues( x=df2$StationID, from=strata$STATIONID, to=strata$Stratum, warn_missing=FALSE )
```
Below, the for-loop prints the unique **StationID** values associated with each **Stratum** in **df2**. We also create a new vector **n.stations.in.strata** to count the number of stations associated with each stratum.
```{r}
# Check that StationID-to-stratum associations are one-to-one or many-to-one
unique.strata <- unique( df2$Stratum )
n.stations.in.strata <- c()
for( i in 1:length(unique.strata) ){
  this.stations <- unique( df2$StationID[which(df2$Stratum==unique.strata[i])] )
  if(length(this.stations)>1){ 
    cat( paste0( unique.strata[i],'::: ',paste(this.stations,collapse='; ',sep=' '),'\n') )
  }  # // end if()
  n.stations.in.strata <- c( n.stations.in.strata, length(this.stations) )  # Confirm one or more stations per stratum
}
```
Printing the unique values of **n.stations.in.strata** indicates that each **Stratum** is associated with one or more **StationID** values.
```{r}
unique(n.stations.in.strata)  # Confirm one or more stations per stratum
```
And similarly, we loop over each unique **StationID** to track the number of strata associated with each station (**n.strata.in.stations**). The output confirms that there is only one stratum per station.
```{r}
# Check that there is no more than one stratum per station
unique.stations <- unique( df2$StationID )
n.strata.in.stations <- c()
for( i in 1:length(unique.stations) ){
  this.stratum <- unique( df2$Stratum[which(df2$StationID==unique.stations[i])] )
  if(length(this.stratum)>1){
    cat( paste0( unique.stations[i],'::: ',paste(this.stratum,collapse='; ',sep=' '),'\n') )
  }  # // end if()
  n.strata.in.stations <- c( n.strata.in.stations, length(this.stratum) )
}  # // end i loop
unique(n.strata.in.stations)  # Confirm no more than one stratum per station
```

[top](#top)


# Remove records flagged with fatal qualifier codes {#Qualifiers}

Fatal qualifier codes indicate unreliable data that should not be used for analysis, but qualifier code definitions may vary by data source. We begin by checking whether any qualifier codes in the dataset stand out as different from FDEP's codes.

[top](#top)


### Qualifier codes and definitions {#Qualifiers1}

First, we print all qualifier codes (**Value_Qualifier** column) associated with each data source.
```{r}
datasources <- unique(df2$DataSource)
for( i in 1:length(datasources) ){
  this.codes <- unique( df2$Value_Qualifier[which(df2$DataSource==datasources[i])] )
  cat( paste0( datasources[i],': ',paste(this.codes,collapse=' ',sep=' '),'\n') )
}  # // end i loop
```
USF Water Atlas provided information on qualifier codes and their definitions according to data sources (**code.defs**); this information was loaded from a csv file at the top of the script. After grouping these codes by data source, and we find that **code.def** includes two data sources that appear in **df2** (**DataSource** column): *USGS_NWIS* and *SWFWMD_HYDRO*. 
```{r}
colnames(code.defs) <- c('DataSource','Code','Meaning')  # Rename columns
code.defs$DataSource[ which(code.defs=='') ] <- 'FDEP'  # Label blank DataSources as 'FDEP', per Water Atlas documentation
codes.by.source <- aggregate( code.defs$Code, by=list(code.defs$DataSource), FUN=unlist, simplify=FALSE ) # Aggregate codes by DataSource
colnames(codes.by.source) <- c('DataSource','Code')  # Rename columns
unique(df2$DataSource)[ which( unique(df2$DataSource) %in% codes.by.source$DataSource ) ]
```
We explore qualifier codes in **df2** associated with these two data sources. The code "P" appears in the *USGS_NWIS* data, and the USGS uses "P" to indicate "preferred" values. These "P"-flagged records can therefore be retained. The *SWFWMD_HYDRO* data have no qualifier code flags, so we also retain all data from this source.
```{r}
df2$Value_Qualifier[ which( df2$DataSource=='USGS_NWIS') ] %>% unique()  # "P"
df2$Value_Qualifier[ which( df2$DataSource=='SWFWMD_HYDRO') ] %>% unique() # None
```

[top](#top)


### Fatal qualifier codes {#Qualifiers2}
  
Next, we remove records flagged with fatal qualifier codes. We specify a vector of fatal codes (**invalid.codes**) and define a *find.invalid()* function to identify fatally flagged records. Because each **Value_Qualifier** string may contain multiple characters, the function parses each string into single characters and checks for matches against the specified **invalid.codes**.
```{r}
invalid.codes <- c("*","?","A","B","G","H","J","K","L","N","O","Q","T","V","Y","Z")
find.invalid <- function( QUALIFIER, INVALID=invalid.codes ){  # QUALIFER arg is a character string
  input.code <- QUALIFIER %>% strsplit(split='') %>% unlist() # parse the string into single characters
  invalid <- input.code %in% INVALID %>% any()  # check if any input characters match INVALID
  return( invalid )  # return TRUE or FALSE
}
```
We apply *find.invalid()* to identify **df2** row indices containing fatal codes, remove these records, and store the result in a new dataframe **df3**.
```{r}
invalid.rows <- apply( matrix(df2$Value_Qualifier,ncol=1), 1, find.invalid ) %>% which()
if( length(invalid.rows)>0 ){
  df3 <- df2[ -invalid.rows, ] 
} else {  
  df3 <- df2
} 
df3$Value_Qualifier <- df3$Value_Qualifier %>% as.factor()
```
A total of `r length(invalid.rows)` fatally flagged records are removed from the dataset.

Finally, we check the remaining qualifier codes and find a few unusual codes:

  * "R1" appears in data from *LEE_WQ*. We contacted the data provider to inquire about this code but received no response. We retain the `r length(which(df3$Value_Qualifier=='R1'))` records flagged with this code.
  * "IC" and "UC" appear in data from *SARASOTA_COASTALCREEK_WQ*. We contacted the data provider to inquire about these codes but received no response. We retain the records flagged with these codes.
```{r}
unique(df3$Value_Qualifier)
df3[ which(df3$Value_Qualifier=='R1'), ]  # LEE_WQ 
df3[ which(df3$Value_Qualifier=='IC'), ]  # SARASOTA_COASTALCREEK_WQ 
df3[ which(df3$Value_Qualifier=='UC'), ]  # SARASOTA_COASTALCREEK_WQ 
```

[top](#top)


# Detection limits and non-detects {#MDL}

Method detection limits (MDLs) vary by analyte and analytical method, and MDLs may vary over time at the same station. In this dataset, many of the records are missing MDL information (values and/or units). For all non-detect samples, we evaluate the available MDL information (**MDL** and **MDL_Unit** columns) for consistency with reported values (**Result_Value** and **Result_Unit** columns) and edit or discard records when justified.

We apply four rules for evaluating/correcting MDLs and non-detects:

1. For non-detect samples that report MDLs in units other than the observation units (**Result_Unit**), we convert the MDLs to the observation units, if possible (e.g. convert mg/l to ug/l for TN). If the units are not convertible, we apply Rule 3.

2. For non-detect samples reporting MDLs without units, we examine each such MDL value and assign a reasonable unit label (**MDL_Unit**), considering the other MDL values associated with the analyte and domain knowledge about the analyte. For instance, if the MDL for a TN sample is reported as 0.010 (without units), we assign "mg/l" rather than "ug/l".

3. For non-detect samples lacking any MDL information, we assess whether the reported **Result_Value** can reasonably be assumed to reflect an unreported MDL. If so, we retain the record. Otherwise, we discard it.

4. For non-detect samples with MDL information reported, we assign the MDL value as the observed value.

We begin by printing the full set of MDL units. We see that missing MDL units are labeled "None" or "" (empty string).
```{r}
df3$MDL_Unit %>% unique() %>% sort()
```
For consistency/simplicity, we replace the "None" label with the empty string.
```{r}
df3$MDL_Unit[ which( df3$MDL_Unit=='None') ] <- ""
```
Next, we identify all non-detect records and label them in a new *logical* column: **Non_detect**. Because qualifier code strings may contain multiple characters, we define a function, *find.U()*, that determines whether a "U" appears in a qualifier code string. For instance, the function will return *TRUE* for "U", "u", and "UI" and *FALSE* for "P", "I", and "IC".
```{r}
# Define function to find records flagged with 'U'
find.U <- function( QUALIFIER, CODE='U' ){  # QUALIFER argument is a character string
  QUALIFIER <- toupper(QUALIFIER)  # capitalize the input
  input.code <- QUALIFIER %>% strsplit(split='') %>%
                           unlist()  # parse the qualifier string into single characters
  u.code <- input.code %in% CODE %>% any()  # check if any input characters contain U
  return( u.code )  # return TRUE or FALSE
}  #  // end find.U() function
# Apply find.U() and store results in new column
df3$Non_detect <- apply( matrix(df3$Value_Qualifier,ncol=1), 1, find.U )
```
We confirm that this labeling procedure was successful with two queries:
```{r}
# Print qualifier codes labeled Non_detect==TRUE
df3$Value_Qualifier[ which( df3$Non_detect==TRUE ) ] %>% unique() %>% as.character()
# Print qualifier codes labeled Non_detect==FALSE
df3$Value_Qualifier[ which( df3$Non_detect==FALSE ) ] %>% unique() %>% as.character()
```

[top](#top)


### Rule 1 {#MDL1}

We evaluate non-detects reporting MDLs and Results in different units. We begin by printing MDL units associated with non-detect samples of each analyte. We find that MDLs for the nitrogen and phosphorus analytes (*TN*, *NH4*, *TKN*, *NOx*, *TP*, and *PO4*) are reported in mg/l, while Results are reported in ug/l.
```{r}
for( i in 1:length(analytes) ){
  this.analyte <- levels(df3$Analyte)[i]
  MDL.units <- df3$MDL_Unit[ which( df3$Analyte==this.analyte &
                                    !is.na(df3$MDL) &
                                    df3$Non_detect==TRUE ) ] %>% unique()
  if( length(MDL.units)==0 ){ MDL.units <- "(No non-detects found)" }
  cat( paste0( this.analyte, ' -- ', MDL.units, '\n' ) )
} 
```
We print all unique nitrogen and phosphorus **MDL** values to confirm that it is reasonable to trust the "mg/l" unit label. The MDL values are all very low (<1), confirming that they are expressed as mg/l. We therefore convert the MDLs from mg/l to ug/l for consistency with the Results units.
```{r}
# Confirm all nitrogen MDL values are mg/l
df3$MDL[ which( df3$Analyte %in% c('TN', 'NH4', 'TKN', 'NOx') &
                !is.na(df3$MDL) & df3$Non_detect==TRUE) ] %>% unique() %>% sort()
# Confirm all phosphorus MDL values are mg/l
df3$MDL[ which( df3$Analyte %in% c('PO4','TP') &
                !is.na(df3$MDL) & df3$Non_detect==TRUE) ] %>% unique() %>% sort()
# Convert nitrogen and phosphorus MDLs from mg/l to ug/l
MDL.idx <- which( df3$Analyte %in% c('TN', 'NH4', 'TKN', 'NOx','PO4','TP') &
                  !is.na(df3$MDL) & df3$Non_detect==TRUE)
df3$MDL[ MDL.idx ] <- df3$MDL[ MDL.idx ] * 1000  # convert mg/l to ug/l
df3$MDL_Unit[ MDL.idx ] <- "ug/l"
```

[top](#top)


### Rule 2 {#MDL2}

We evaluate non-detects reporting MDL values without units. For each analyte, we print the **MDL** values that are missing units ("target"), along with the full set of **MDL** values, whether labeled or unlabeled ("all"). We find that for four analytes (*BOD*, *Chl-a*, *Fecal*, and *TSS*), some reported **MDL** values are missing units. Because each of these **MDL** values fits well within the corresponding set of all MDLs, we assume that for each analyte, the **MDL** values that are missing unit labels are expressed in the same units as **MDL** values with labeled units.
```{r}
for( i in 1:length(analytes) ){
  this.analyte <- analytes[i]
  targets.idx <- which( df3$Analyte==this.analyte &
                        !is.na(df3$MDL) &
                        df3$MDL_Unit=='' &
                        df3$Non_detect==TRUE )
  if( length(targets.idx)==0 ){
    targets.MDL <- "(No values found)"
    all.MDL <- "(skip)"
  } else {
    targets.MDL <- df3$MDL[ targets.idx ] %>% unique() %>% sort()
    all.MDL <- df3$MDL[ which( df3$Analyte==this.analyte &
                               !is.na(df3$MDL) ) ] %>% unique() %>% sort()
  }  # end if()
  cat( paste('===============', this.analyte, '=========================\n') )
  cat( paste('Target MDL values:',  paste(targets.MDL,collapse=', '), '\n') )
  cat( paste('All MDL values:', paste(    all.MDL,collapse=', '), '\n\n') )
} 
```
We assign unit labels where they are missing. The four **analytes** with some missing MDL units are specified in **analytes.rule2**, and the corresponding units are specified in **units.rule2**.
```{r}
analytes.rule2 <- c('BOD','Chl-a','Fecal','TSS')
units.rule2 <- c('mg/l','ug/l','cfu/100ml','mg/l')
for( i in 1:length(analytes.rule2) ){
  this.analyte <- analytes.rule2[i]
  targets.idx <- which( df3$Analyte==this.analyte &
                        !is.na(df3$MDL) &
                        df3$MDL_Unit=='' &
                        df3$Non_detect==TRUE )
  df3$MDL_Unit[ targets.idx ] <- units.rule2[i] 
}
```    

[top](#top)
 
 
### Rule 3 {#MDL3}    

We evaluate non-detects with no MDL information (no values or units). For each analyte, we print the "target" Result values and value range (values with no associated MDL information), the MDL values associated with non-detects, and all MDL values. We find some unexpected Result values in *Color*, *NH4*, and *TKN*.
```{r}  
for( i in 1:length(analytes)){
  this.analyte <- analytes[i]
  targets.idx <- which( df3$Analyte==this.analyte &
                          is.na(df3$MDL) &
                          df3$Non_detect==TRUE )
  if( length(targets.idx)==0 ){
    targets.results <- "(No Values found)"
    targets.range   <- "(skip)"
    all.nondetect.MDL <- "(skip)"
    all.MDL <- "(skip)"
  } else {
    targets.results <- df3$Result_Value[ targets.idx ] %>% unique() %>% sort()
    targets.range   <- range( targets.results )
    all.nondetect.MDL <- df3$MDL[ which( df3$Analyte==this.analyte & df3$Non_detect==TRUE &
                                           !is.na(df3$MDL) ) ] %>% unique() %>% sort()
    all.MDL <- df3$MDL[ which( df3$Analyte==this.analyte & 
                                 !is.na(df3$MDL) ) ] %>% unique() %>% sort()
  }  # // end if()
  cat( paste('===============', this.analyte, '=========================\n') )
  cat( paste('Target Result values:', paste(targets.results,collapse=', '), '\n') )
  cat( paste('Target Result range: ', paste(  targets.range,collapse=' - '), '\n\n') )
  cat( paste('All non-detect MDL values:', paste(all.nondetect.MDL,collapse=', '), '\n\n') )
  cat( paste('All MDL values:', paste(all.MDL,collapse=', '), '\n\n') )
}
```
We explore the large (>8) Result values among non-detect *Color* samples.
```{r}    
df3[ which( df3$Analyte=='Color' &
              df3$Result_Value>8 & df3$Non_detect==TRUE), ]
```
Each of these `r length(which( df3$Analyte=='Color' & df3$Result_Value>8 & df3$Non_detect==TRUE))` *Color* records cites elevated MDL due to sample matrix interference (**Result_Comment** column). Therefore, we discard these records. 
```{r}
df3 <- df3[ -which( df3$Analyte=='Color' &
                    df3$Result_Value>8 & df3$Non_detect==TRUE), ]
```
Next, we explore the low Result values (0.008) among non-detect *NH4* samples. We find `r length(which(df3$Analyte=='NH4' & df3$Result_Value==0.008 & df3$Non_detect==TRUE))` such records, and they are all labeled with "ug/l" units.
```{r}    
NH4.008.idx <- which( df3$Analyte=='NH4' &
                      df3$Result_Value==0.008 & df3$Non_detect==TRUE)
df3$Result_Unit[ NH4.008.idx ] %>% unique()  # units say 'ug/l'
```
Because an MDL of 0.008 ug/l is not realistic, but an MDL of 0.008 mg/l would make sense, we assume the "ug/l" label is a mistake and convert the value from mg/l to ug/l. That is, we assign an MDL of 8 ug/l for each of the `r length(NH4.008.idx)` records.
```{r}
df3$Result_Value[ NH4.008.idx ] <- df3$Result_Value[ NH4.008.idx ] * 1000  # mg/l to ug/l
```
And finally, we explore the large Result values (>100) among non-detect *TKN* samples.
```{r}
df3[ which( df3$Analyte=='TKN' &
            df3$Result_Value>100 & df3$Non_detect==TRUE), ]
```
Whereas MDLs of 160-640 ug/l might be realistic, an MDL of 1100 ug/l seems highly unlikely. We therefore discard these `r length(which(df3$Analyte=='TKN' & df3$Result_Value==1100 & df3$Non_detect==TRUE))` records.
```{r}
df3 <- df3[ -which( df3$Analyte=='TKN' &
                    df3$Result_Value==1100 & df3$Non_detect==TRUE), ]
```

[top](#top)


### Rule 4 {#MDL4}

We identify all non-detects with MDL information and check whether the Result values match the corresponding MDL values.
```{r}    
 mismatch.idx <- which( df3$Non_detect==TRUE &
                        !is.na(df3$MDL) &
                        df3$MDL!=df3$Result_Value )
```
We find `r length(mismatch.idx)` records for which the **Result_Value** does not equal the **MDL**.
```{r}
df3[ mismatch.idx, ]  # 3 records found
```
For these `r length(mismatch.idx)` records, we assign the MDL value as the Result value.
```{r}
df3$Result_Value[ mismatch.idx ] <- df3$MDL[ mismatch.idx ]
```

[top](#top)


# Apply thresholds to result values {#Thresholds}

We explore the data distributions for each analyte to identify suspect values, and we discard values outside specified minimum and maximum thresholds.

We begin by visualizing the data distributions for each analyte. The minimum and maximum reported values (**Result_Value**) are printed near the top right corner of each histogram.
```{r fig.width=10, fig.height=8}
par(mfrow=c(3,3))
for( i in 1:length(analytes) ){
  this.analyte <- analytes[i]
  this.unit    <- unique( df3$Result_Unit[ which(df3$Analyte==this.analyte) ] )
  # cat(paste0(i,'. ',this.analyte,": ",this.unit,'\n'))  # print units to console
  if(this.unit[1]=="None"){ this.unit<-'' }
  hist( df3$Result_Value[ which(df3$Analyte==this.analyte) ], breaks=50,
        border=rgb(1,1,1,1), col=rgb(0,0.2,0.9,1),
        main=this.analyte, xlab=this.unit
        )
  legend('topright',bty='n',legend=range(df3$Result_Value[which(df3$Analyte==this.analyte)]))
}
```
The threshold values are stored in the *thresholds.csv* file, which was loaded as the **threshold** dataframe at the top of this script. The **low** and **high** columns contain the threshold values for each analyte, and the **comment** column (not printed below) describes the rationale for each choice (open *thresholds.csv* to view these comments). Specification of thresholds was informed by the data distributions (see histograms above) and by domain knowledge. For several analytes, no maximum threshold is specified, to avoid discarding unusually large albeit plausible reported values. 
```{r}
thresholds[,c('analyte','low','high','unit')]
```
Next, we screen records with Result values that fall outside the specified thresholds. We initialize a new **threshold.screening** dataframe to keep track of the number of discarded records, loop over each analyte to identify and discard values outside the thresholds, and write **threshold.screening** to file (*threshold.results.csv*). 
```{r}
    threshold.screening <- data.frame( analyte=analytes, n.removals=NA )
    # Loop over analytes and screen out data outside of specified thresholds
    for( i in 1:length(analytes) ){
      # Verify threshold units match data units
      this.unit <- unique(df3$Result_Unit[which(df3$Analyte==analytes[i])])
      if( thresholds$unit[i] != this.unit ){
        stop(paste("Threshold unit doesn't match data (",analytes[i],")."))
        } # // end if()
      # Remove data outside of specified thresholds
      outside.idx <- which( df3$Analyte==analytes[i] &
                            (df3$Result_Value<thresholds$low[i] | df3$Result_Value>thresholds$high[i]) )
      if( length(outside.idx)>0 ){
        df3 <- df3[-outside.idx,]
      }  # // end if()
      # Add summary entry
      threshold.screening$n.removals[i] <- length(outside.idx)
    }  # // end i loop
    # Write summary to file
    write.csv(threshold.screening,"threshold.results.csv",row.names=FALSE)
```
Below, we print **threshold.screening**, which indicates the total number of records discarded for each analyte. A small number of *DO Conc*, *DO Sat*, *pH*, *Sp Cond*, and *TSS* records were discarded.
```{r}
threshold.screening
```
Finally, we re-run the plotting loop above to visualize the data distributions following the threshold screening. Comparing the two sets of histograms, we observe that the distributions for the affected analytes have changed.
```{r fig.width=10, fig.height=8}
par(mfrow=c(3,3))
for( i in 1:length(analytes) ){
  this.analyte <- analytes[i]
  this.unit    <- unique( df3$Result_Unit[ which(df3$Analyte==this.analyte) ] )
  # cat(paste0(i,'. ',this.analyte,": ",this.unit,'\n'))  # print units to console
  if(this.unit[1]=="None"){ this.unit<-'' }
  hist( df3$Result_Value[ which(df3$Analyte==this.analyte) ], breaks=50,
        border=rgb(1,1,1,1), col=rgb(0.9,0.2,0,1),
        main=this.analyte, xlab=this.unit
        )
  legend('topright',bty='n',legend=range(df3$Result_Value[which(df3$Analyte==this.analyte)]))
}
```

[top](#top)


# Export clean data {#Export}

We export the clean dataset to a csv file.
```{r}
write.csv( df3, "CHNEP-WQ_clean-data_2011-2020 (Rmd).csv", row.names=FALSE )
```

[top](#top)

# References

Dowle, M. & Srinivasan, A. (2021). data.table: Extension of `data.frame`. R package version 1.14.0. [https://CRAN.R-project.org/package=data.table](https://CRAN.R-project.org/package=data.table)

R Core Team (2021). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. [https://www.R-project.org/](https://www.R-project.org/)

Wickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. *Journal of Statistical Software*, 40(1): 1-29. [http://www.jstatsoft.org/v40/i01/](http://www.jstatsoft.org/v40/i01/)

Wickham, H. (2021). tidyr: Tidy Messy Data. R package version 1.1.3. [https://CRAN.R-project.org/package=tidyr](https://CRAN.R-project.org/package=tidyr)

Wickham, H., François, R., Henry, L. & Müller, K. (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.6. [https://CRAN.R-project.org/package=dplyr](https://CRAN.R-project.org/package=dplyr)

[top](#top)
  
  
  